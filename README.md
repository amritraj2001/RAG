Retrieval-Augmented Generation (RAG) Pipeline

This project implements a Retrieval-Augmented Generation (RAG) system where users can upload documents, query them using natural language, and get contextual responses generated by LLMs. The system is containerized with Docker Compose and works on both local and cloud environments.

1. Setup & Installation
Clone Repository
Environment Variables: Create a .env file in the root directory (sample already provided).
Local Installation
Run with Docker Compose
2. API Usage & Testing
Base URL (local)
Endpoints
Upload Documents
Query Documents
Get Document Metadata
Testing: Run unit & integration tests
3. Configuration for Different LLM Providers
The system supports multiple LLM backends. Configure provider in .env file:
ðŸ”¹ OpenAI
ðŸ”¹ HuggingFace
ðŸ”¹ Groq
The application will automatically switch to the selected provider when processing queries.
